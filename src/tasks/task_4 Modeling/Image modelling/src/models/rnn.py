from torch import nn
import torch

import math
from pathlib import Path

from .base import ModelBase


class RNNModel(ModelBase):
    """
    A PyTorch implementaion of RNN structured model from the original paper. Note that
    
    """

    def __init__(self,rnn_params,gausian_params):
    
        dense_features = rnn_params["dense_features"]
        device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        model = RNNet(rnn_params)
        
        if dense_features == "None":
            num_dense_layers = 2
        else:
            num_dense_layers = len(dense_features)
        model_weight = f"dense_layers.{num_dense_layers - 1}.weight"
        model_bias = f"dense_layers.{num_dense_layers - 1}.bias"
        
        super().__init__(
            model,
            model_weight,
            model_bias,
            "rnn",
            rnn_params['savedir'],
            gausian_params
        )

    def reinitialize_model(self, time=None):
        self.model.initialize_weights()


class RNNet(nn.Module):
    """
    A crop yield conv net.
 """

    def __init__(self,rnn_params):
    
        super().__init__()
        dense_features = rnn_params['dense_features']
        if dense_features == "None":
            dense_features = [256, 1]
        dense_features.insert(0, rnn_params["hidden_size"])
         
        self.dropout = nn.Dropout(rnn_params["rnn_dropout"])
        self.rnn = nn.LSTM(
            input_size=rnn_params["in_channels"] * rnn_params["num_bins"],
            hidden_size=rnn_params["hidden_size"],
            num_layers=rnn_params["num_rnn_layers"],
            batch_first=True,
        )
        self.hidden_size = rnn_params["hidden_size"]

        self.dense_layers = nn.ModuleList(
            [
                nn.Linear(
                    in_features=dense_features[i - 1], out_features=dense_features[i]
                )
                for i in range(1, len(dense_features))
            ]
        )

        self.initialize_weights()

    def initialize_weights(self):

        sqrt_k = math.sqrt(1 / self.hidden_size)
        for parameters in self.rnn.all_weights:
            for pam in parameters:
                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)

        for dense_layer in self.dense_layers:
            nn.init.kaiming_uniform_(dense_layer.weight.data)
            nn.init.constant_(dense_layer.bias.data, 0)

    def forward(self, x, return_last_dense=False):
        """
        If return_last_dense is true, the feature vector generated by the second to last
        dense layer will also be returned. This is then used to train a Gaussian Process model.
        """
        # the model expects feature_engineer to have been run with channels_first=True, which means
        # the input is [batch, bands, times, bins].
        # Reshape to [batch, times, bands * bins]
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(x.shape[0], x.shape[1], x.shape[2] * x.shape[3])

        sequence_length = x.shape[1]

        hidden_state = torch.zeros(1, x.shape[0], self.hidden_size)
        cell_state = torch.zeros(1, x.shape[0], self.hidden_size)

        if x.is_cuda:
            hidden_state = hidden_state.cuda()
            cell_state = cell_state.cuda()

        for i in range(sequence_length):
            # The reason the RNN is unrolled here is to apply dropout to each timestep;
            # The rnn_dropout argument only applies it after each layer. This better mirrors
            # the behaviour of the Dropout Wrapper used in the original repository
            # https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper
            input_x = x[:, i, :].unsqueeze(1)
            _, (hidden_state, cell_state) = self.rnn(
                input_x, (hidden_state, cell_state)
            )
            hidden_state = self.dropout(hidden_state)

        x = hidden_state.squeeze(0)
        for layer_number, dense_layer in enumerate(self.dense_layers):
            x = dense_layer(x)
            if return_last_dense and (layer_number == len(self.dense_layers) - 2):
                output = x
        if return_last_dense:
            return x, output
        return x
